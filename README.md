ðŸ“˜ GestureTalk AI

<<<<<<< HEAD
##

## GestureTalk AI is an intelligent real-time sign language recognition system built to improve communication between the deaf community and the hearing population. Using MediaPipe, TensorFlow, scikit-learn, and OpenCV, the system detects hand gestures via webcam and converts them instantly into text and speech.

##
=======
GestureTalk AI is a simple tool that recognizes hand gestures using AI. It uses a webcam to detect signs and converts them into text and speech. The goal is to help improve communication for people with hearing impairments.

ðŸš€ Features

Real-time hand tracking
>>>>>>> a2a6c9a339a346dbcd9ad471fb55f0eda8cdc99c

Fast and accurate gesture recognition

<<<<<<< HEAD
##

## âœ‹ Real-time gesture tracking using MediaPipe Hand Landmark Detection

## 

## ðŸ§  Custom-trained ML model for accurate sign classification

## 

## ðŸŽ¤ Text-to-Speech output for smooth communication

## 

## ðŸŽ¯ Supports multiple gestures with high accuracy

## 

## ðŸª¶ Lightweight, fast, and optimized for real-time performance

## 

## ðŸ§© Easy to extend with new gestures and models

## 
=======
Converts signs to text

Optional speech output

Easy to train and add new gestures

ðŸ› ï¸ Technologies Used

Python

MediaPipe

TensorFlow

OpenCV

NumPy

scikit-learn



ðŸš€ How to Run the Project
âœ… Step 1: Install Dependencies
>>>>>>> a2a6c9a339a346dbcd9ad471fb55f0eda8cdc99c

Run this command in your terminal:

<<<<<<< HEAD
## 

## Languages: Python

## Libraries:

## 

## MediaPipe

## 

## TensorFlow

## 

## scikit-learn

## 

## NumPy

##

## OpenCV

## 

## pyttsx3 (for speech output)

## 

# ðŸ“ Project Structure

## GestureTalk-AI/

## â”‚â”€â”€ data/               # Dataset for gesture training

## â”‚â”€â”€ model/              # Saved ML model files

## â”‚â”€â”€ src/                # Main program code

## â”‚   â”œâ”€â”€ collect\_data.py

## â”‚   â”œâ”€â”€ train\_model.py

## â”‚   â”œâ”€â”€ recognize.py

## â”‚â”€â”€ README.md

## â”‚â”€â”€ requirements.txt

## 

# âš™ï¸ How It Works

### 

## MediaPipe detects hand landmarks (21 coordinates per hand).

###

## Coordinates are converted into a numerical dataset.

### 

## A TensorFlow model is trained to classify gestures.

### 

## In real time, the model predicts the gesture shown to the camera.

### 

## The system displays the prediction and optionally speaks it aloud.
# ðŸš€ How to Run the Project
## âœ… Step 1: Install Dependencies
=======
pip install -r requirements.txt

âœ… Step 2: Create the Dataset

Use the trainer script to collect gesture data:

python trainer.py

âœ… Step 3: Train & Generate the Model

After collecting data, train your model:

python createModel.py


>>>>>>> a2a6c9a339a346dbcd9ad471fb55f0eda8cdc99c

### Run this command in your terminal:

- `pip install -r requirements.txt`

## âœ… Step 2: Create the Dataset

### Use the trainer script to collect gesture data:

- `python trainer.py`

## âœ… Step 3: Train & Generate the Model

### After collecting data, train your model:

- `python createModel.py`
